{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coati.training.train_coati import train_autoencoder, do_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning... unparsed:  ['--ip=127.0.0.1', '--stdin=9003', '--control=9001', '--hb=9000', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"941f33cc-99b3-4dd8-8ae5-3b98b55f6c15\"', '--shell=9002', '--transport=\"tcp\"', '--iopub=9004', '--f=/home/haotian/.local/share/jupyter/runtime/kernel-v2-745785e5fgQOqStENy.json']\n"
     ]
    }
   ],
   "source": [
    "args = do_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\" # Temporary setup, single GPU training only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning... unparsed:  ['--ip=127.0.0.1', '--stdin=9003', '--control=9001', '--hb=9000', '--Session.signature_scheme=\"hmac-sha256\"', '--Session.key=b\"941f33cc-99b3-4dd8-8ae5-3b98b55f6c15\"', '--shell=9002', '--transport=\"tcp\"', '--iopub=9004', '--f=/home/haotian/.local/share/jupyter/runtime/kernel-v2-745785e5fgQOqStENy.json']\n",
      "running on 1 gpus\n"
     ]
    }
   ],
   "source": [
    "args = do_args()\n",
    "args.nodes = 1  # total num nodes.\n",
    "args.nr = 0  # rank of this node.\n",
    "# note args.gpus will default to the # gpus on this node.\n",
    "args.data_parallel = True ####\n",
    "\n",
    "args.test_frac = 0.02\n",
    "args.valid_frac = 0.0\n",
    "args.n_layer_e3gnn = 5\n",
    "args.n_hidden_e3nn = 256\n",
    "args.msg_cutoff_e3nn = 12.0\n",
    "args.n_hidden_xformer = 256\n",
    "args.n_embd_common = 256\n",
    "args.n_layer_xformer = 16\n",
    "args.n_head = 16\n",
    "args.max_n_seq = 250  # max the model can forward\n",
    "#    args.n_seq = 90 # max allowed in training.\n",
    "args.n_seq = 64  # max allowed in training.\n",
    "args.biases = True\n",
    "args.torch_emb = False\n",
    "args.norm_clips = True\n",
    "args.norm_embed = False\n",
    "args.token_mlp = True\n",
    "\n",
    "args.tokenizer_vocab = \"mar\"\n",
    "args.p_dataset = 0.2\n",
    "args.p_formula = 0.0\n",
    "args.p_fim = 0.0\n",
    "args.p_graph = 0.0\n",
    "args.p_clip = 0.9\n",
    "args.p_clip_emb_smi = 0.5\n",
    "args.p_randsmiles = 0.3\n",
    "args.batch_size = 160\n",
    "\n",
    "args.online = False  # Possible offline training of an end-to-end clip\n",
    "args.lr = 5.0e-4\n",
    "args.weight_decay = 0.1\n",
    "\n",
    "args.dtype = \"float\"\n",
    "args.n_epochs = 100\n",
    "args.clip_grad = 10\n",
    "args.test_interval = 2\n",
    "args.debug = False\n",
    "\n",
    "args.resume_optimizer = False\n",
    "# resume from checkpoint file\n",
    "args.resume_document = None\n",
    "\n",
    "args.ngrad_to_save = 6e6\n",
    "\n",
    "# output logs\n",
    "args.output_dir = \"./logs/\"\n",
    "# where to save model checkpoints\n",
    "args.model_dir = \"./model_ckpts/\"\n",
    "# where to save dataset cache\n",
    "args.data_dir = \"./data/ecloud_coati.h5\"\n",
    "args.model_filename = \"coati_grande\"\n",
    "\n",
    "print(f\"running on {args.gpus} gpus\")\n",
    "#########################################################\n",
    "args.world_size = args.gpus * args.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train autoencoder rank 0 reporting in.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_autoencoder(args\u001b[39m.\u001b[39;49mgpus, args)\n",
      "File \u001b[0;32m~/Molecule_Generation/MG/backupECloud/EcloudGen-COATI/coati/training/train_coati.py:74\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(gpu, args)\u001b[0m\n\u001b[1;32m     72\u001b[0m local_rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mLOCAL_RANK\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     73\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(local_rank)\n\u001b[0;32m---> 74\u001b[0m dist\u001b[39m.\u001b[39;49minit_process_group(\n\u001b[1;32m     75\u001b[0m     backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnccl\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     76\u001b[0m     \u001b[39m# init_method=\"env://\",\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39m# world_size=args.world_size,  # Computed by the mp.spawn caller.\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m     \u001b[39m# rank=rank,\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m rank \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     82\u001b[0m     output_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39moutput_dir, args\u001b[39m.\u001b[39mexp_name, args\u001b[39m.\u001b[39mrun_name)\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecloud/lib/python3.9/site-packages/torch/distributed/c10d_logger.py:74\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     73\u001b[0m     t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime_ns()\n\u001b[0;32m---> 74\u001b[0m     func_return \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime_ns()\n\u001b[1;32m     77\u001b[0m     \u001b[39mif\u001b[39;00m dist\u001b[39m.\u001b[39mis_initialized():\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecloud/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:1141\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[39mif\u001b[39;00m store \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1138\u001b[0m     rendezvous_iterator \u001b[39m=\u001b[39m rendezvous(\n\u001b[1;32m   1139\u001b[0m         init_method, rank, world_size, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     store, rank, world_size \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(rendezvous_iterator)\n\u001b[1;32m   1142\u001b[0m     store\u001b[39m.\u001b[39mset_timeout(timeout)\n\u001b[1;32m   1144\u001b[0m     \u001b[39m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[39m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecloud/lib/python3.9/site-packages/torch/distributed/rendezvous.py:231\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[0;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(query_dict[\u001b[39m\"\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     rank \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(_get_env_or_raise(\u001b[39m\"\u001b[39;49m\u001b[39mRANK\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mworld_size\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m query_dict:\n\u001b[1;32m    234\u001b[0m     world_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(query_dict[\u001b[39m\"\u001b[39m\u001b[39mworld_size\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/software/miniconda3/envs/ecloud/lib/python3.9/site-packages/torch/distributed/rendezvous.py:216\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[0;34m(env_var)\u001b[0m\n\u001b[1;32m    214\u001b[0m env_val \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(env_var, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m env_val:\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m _env_error(env_var)\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m env_val\n",
      "\u001b[0;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "train_autoencoder(args.gpus, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
